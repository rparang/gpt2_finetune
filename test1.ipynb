{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ac260a-6a66-417b-9cea-3920a1692e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "class DataLoaderLite:\n",
    "\tdef __init__(self, B, T):\n",
    "\t\tself.B = B\n",
    "\t\tself.T = T\n",
    "\n",
    "\t\t# at init load tokens from disk and store them in memory\n",
    "\t\twith open('input.txt', 'r') as f:\n",
    "\t\t\ttext = f.read()\n",
    "\t\tenc = tiktoken.get_encoding('gpt2')\n",
    "\t\ttokens = enc.encode(text)\n",
    "\t\tself.tokens = torch.tensor(tokens)\n",
    "\t\tprint(f\"loaded {len(self.tokens)} tokens\")\n",
    "\t\tprint(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "\t\t# state\n",
    "\t\tself.current_position = 0\n",
    "\n",
    "\tdef next_batch(self):\n",
    "\t\tB, T = self.B, self.T\n",
    "\t\tbuf = self.tokens[self.current_position:self.current_position + B * T + 1]\n",
    "\t\tx = (buf[:-1]).view(B, T) # inputs\n",
    "\t\ty = (buf[1:]).view(B, T) # targets\n",
    "\n",
    "\t\tself.current_position += B * T\n",
    "\n",
    "\t\tif self.current_position + (B * T + 1) > len(self.tokens):\n",
    "\t\t\tself.current_position = 0\n",
    "\t\treturn x, y\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert config.n_embd % config.n_head == 0\n",
    "\n",
    "\t\tself.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # key, query, value projections for all heads but in a batch. Saves you from three separate instantiations of nn.Linear\n",
    "\t\tself.c_proj = nn.Linear(config.n_embd, config.n_embd) # output projection\n",
    "\t\tself.c_proj.NANOGPT_SCALE_INIT = 1 # set flag so we know on initialization we need to scale down the std for these residual streams\n",
    "\n",
    "\t\tself.n_head = config.n_head\n",
    "\t\tself.n_embd = config.n_embd\n",
    "\n",
    "\t\tself.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "\tdef forward(self, x, attention_mask=None):\n",
    "\t\n",
    "\t\tB, T, C = x.size() # batch size, sequence length, embedding dimension (n_embd)\n",
    "\t\n",
    "\t\t# Calculate query, key, value for all heads in batch, move head forward in the shape to be a batch dim alongside B\n",
    "\t\t# nh is \"number of heads\", hs is \"head size\", and C is number of channels (nh * hs)\n",
    "\t\t# e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs = 768 channels in the Transformer\n",
    "\t\n",
    "\t\tqkv = self.c_attn(x)\n",
    "\t\tq, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\t\tk = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\t\tq = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\t\tv = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\t\n",
    "\t\t# attention materializes the large (T, T) matrix for all queries and keys\n",
    "\t\tatt = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.size(-1))) # --> (B, nh, T, T)\n",
    "\t\n",
    "\t\t# apply causal mask\n",
    "\t\tatt = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "\t\n",
    "\t\t# apply padding mask if needed\n",
    "\t\tif attention_mask is not None:\n",
    "\t\t\tattention_mask = attention_mask[:, None, None, :] # (B, T) --> (B, 1, 1, T)\n",
    "\t\t\tatt = att.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\t\t\n",
    "\t\tatt = F.softmax(att, dim=-1)\n",
    "\t\n",
    "\t\ty = att @ v # (B, nh, T, T) x (B, nh, T, hs) --> (B, nh, T, hs)\n",
    "\t\ty = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\t\n",
    "\t\t# output project\n",
    "\t\ty = self.c_proj(y)\n",
    "\t\treturn y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd) # On naming (eg 'c_fc'), we are replicating the GPT2 model\n",
    "\t\tself.gelu = nn.GELU(approximate='tanh')\n",
    "\t\tself.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\t\tself.c_proj.NANOGPT_SCALE_INIT = 1 # set flag so we know on initialization we need to scale down the std for these residual streams\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.c_fc(x)\n",
    "\t\tx = self.gelu(x)\n",
    "\t\tx = self.c_proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "\t\tself.attn = CausalSelfAttention(config)\n",
    "\t\tself.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "\t\tself.mlp = MLP(config)\t\t\n",
    "\n",
    "\tdef forward(self, x, attention_mask=None):\n",
    "\t\tx = x + self.attn(self.ln_1(x), attention_mask=attention_mask)\n",
    "\t\tx = x + self.mlp(self.ln_2(x)) \n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\tblock_size: int = 1024 # max sequence length\n",
    "\tvocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "\tn_layer: int = 12 # number of layers\n",
    "\tn_head: int = 12 # number of heads\n",
    "\tn_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\tself.transformer = nn.ModuleDict(dict(\n",
    "\t\t\twte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "\t\t\twpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "\t\t\th = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "\t\t\tln_f = nn.LayerNorm(config.n_embd)\n",
    "\t\t))\n",
    "\t\tself.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "\t\t# weight sharing scheme\n",
    "\t\tself.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, module):\n",
    "\t\tif isinstance(module, nn.Linear):\n",
    "\t\t\tstd = 0.02\n",
    "\t\t\tif hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "\t\t\t\tstd *= (2 * self.config.n_layer) ** -0.5 # Scale down the residual streams so std doesn't bloat as the streams add. Note we multiply by 2 bc it happens twice in each Block (one residual in attention, one in MLP)\n",
    "\t\t\ttorch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\t\t\tif module.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.zeros_(module.bias)\n",
    "\t\telif isinstance(module, nn.Embedding):\n",
    "\t\t\ttorch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\tdef resize_token_embeddings_and_tie_weights(self, new_vocab_size):\n",
    "\t\n",
    "\t\told_weight = self.transformer.wte.weight.data\n",
    "\t\told_vocab_size, n_embd = old_weight.shape # Get current size\n",
    "\n",
    "\t\tassert new_vocab_size > old_vocab_size, f\"New vocab size is not larger than current vocab size\"\n",
    "\t\t\n",
    "\t\t# Create new embedding layer and copy weights\n",
    "\t\tself.transformer.wte = nn.Embedding(new_vocab_size, n_embd)\n",
    "\t\tself.transformer.wte.weight.data[:old_vocab_size] = old_weight\n",
    "\t\t# nn.init.normal_(self.transformer.wte.weight.data[old_vocab_size:], mean=0.0, std=0.02)\n",
    "\t\t\n",
    "\t\t# Comment this out to illustrate bad initialization\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\taverage = self.transformer.wte.weight[:old_vocab_size].mean(dim=0)\n",
    "\t\t\tself.transformer.wte.weight.data[old_vocab_size:] = average\n",
    "\t\t\n",
    "\t\t# Create new lm_head layer\n",
    "\t\tself.lm_head = nn.Linear(n_embd, new_vocab_size, bias=False)\n",
    "\t\t\n",
    "\t\t# Tie weights\n",
    "\t\tself.lm_head.weight = self.transformer.wte.weight\n",
    "\n",
    "\t\tprint(f\"Model resized {model.transformer.wte} and {model.lm_head} layers to {new_vocab_size}\")\n",
    "\n",
    "\tdef forward(self, idx, targets=None, attention_mask=None):\n",
    "\t\t# idx is shape (B, T)\n",
    "\t\tB, T = idx.size()\n",
    "\t\tassert T <= self.config.block_size, f\"Cannot forward sequence of length {T}. Block size is only {self.config.block_size}\"\n",
    "\t\t\n",
    "\t\t# forward the token and position embeddings\n",
    "\t\tpos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "\t\tpos_emb = self.transformer.wpe(pos) # shape (T, n_embd)\n",
    "\t\ttok_emb = self.transformer.wte(idx) # shape (B, T, n_embd)\n",
    "\t\tx = tok_emb + pos_emb\n",
    "\t\t\n",
    "\t\t# forward through the blocks of the transformer\n",
    "\t\tfor block in self.transformer.h:\n",
    "\t\t\tx = block(x, attention_mask=attention_mask)\n",
    "\t\t\n",
    "\t\t# forward the final layernorm and the classifier\n",
    "\t\tx = self.transformer.ln_f(x)\n",
    "\t\tlogits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif targets is not None:\n",
    "\t\t\tloss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\t\treturn logits, loss\n",
    "\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_pretrained(cls, model_type):\n",
    "\t\t\"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "\t\tassert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\t\tfrom transformers import GPT2LMHeadModel\n",
    "\t\tprint(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "\t\t# n_layer, n_head and n_embd are determined from model_type\n",
    "\t\tconfig_args = {\n",
    "\t\t\t'gpt2':\t\t\tdict(n_layer=12, n_head=12, n_embd=768), \t# 124M params\n",
    "\t\t\t'gpt2-medium':\tdict(n_layer=24, n_head=16, n_embd=1024), \t# 350M params\n",
    "\t\t\t'gpt2-large':\tdict(n_layer=36, n_head=20, n_embd=1280), \t# 774M param\n",
    "\t\t\t'gpt2-xl':\t\tdict(n_layer=48, n_head=25, n_embd=1600), \t# 1558M params\n",
    "\t\t}[model_type]\n",
    "\t\tconfig_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "\t\tconfig_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "\n",
    "\t\t# create a from-scratch initialized minGPT model\n",
    "\t\tconfig = GPTConfig(**config_args)\n",
    "\t\tmodel = GPT(config)\n",
    "\t\tsd = model.state_dict()\n",
    "\t\tsd_keys = sd.keys()\n",
    "\t\tsd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # dicard the mask / buffer\n",
    "\n",
    "\t\t# init a huggingface/transformers model\n",
    "\t\tmodel_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "\t\tsd_hf = model_hf.state_dict()\n",
    "\n",
    "\t\t# copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "\t\tsd_keys_hf = sd_hf.keys()\n",
    "\t\tsd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # dicard the mask / buffer\n",
    "\t\tsd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # dicard the mask / buffer\n",
    "\t\ttransposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\t\t# basically the openai checkppoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "\t\t# this means we have to transpose these weights when we import them\n",
    "\t\tassert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "\t\tfor k in sd_keys_hf:\n",
    "\t\t\tif any(k.endswith(w) for w in transposed):\n",
    "\t\t\t\t# special treatment for the Conv1D weights we need to transpose\n",
    "\t\t\t\tassert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tsd[k].copy_(sd_hf[k].t())\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert sd_hf[k].shape == sd[k].shape\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tsd[k].copy_(sd_hf[k])\n",
    "\n",
    "\t\treturn model\n",
    "\n",
    "\tdef generate(self, device, max_length=50, num_return_sequences=1, query=\"Best tacos?\", tokenizer=None):\n",
    "\n",
    "\t\tif tokenizer is None:\n",
    "\t\t\tfrom transformers import GPT2Tokenizer\n",
    "\t\t\ttokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\t\t\n",
    "\t\ttokens = tokenizer.encode(query)\n",
    "\t\ttokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\t\ttokens = tokens.unsqueeze(0)\n",
    "\t\ttokens = tokens.repeat(num_return_sequences,1)\n",
    "\t\tx = tokens\n",
    "\t\tx = x.to(device)\n",
    "\t\t\n",
    "\t\twhile x.size(1) < max_length:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tlogits, loss = self(x)\n",
    "\t\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\t\tprobs = F.softmax(logits, dim=-1)\n",
    "\t\t\t\ttopk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\t\t\t\tix = torch.multinomial(topk_probs, 1)\n",
    "\t\t\t\txcol = torch.gather(topk_indices, -1, ix)\n",
    "\t\t\t\tx = torch.cat((x, xcol), dim=1)\n",
    "\t\t\n",
    "\t\t\t\tfor i in range(num_return_sequences):\n",
    "\t\t\t\t\ttokens = x[i, :].tolist()\n",
    "\t\t\t\t\tdecoded = tokenizer.decode(tokens)\n",
    "\t\t\t\t\tprint(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d8c81f-e84c-4de6-8393-84d9fd99ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class QADataLoader:\n",
    "    def __init__(self, filepath, max_length=512, shuffle=True):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.tokenizer.add_special_tokens({\n",
    "            \"bos_token\": \"<BOS>\",\n",
    "            \"eos_token\": \"<EOS>\",\n",
    "            \"sep_token\": \"<SEP>\",\n",
    "            \"pad_token\": \"<PAD>\"\n",
    "        })\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<BOS>\": self.tokenizer.encode(\"<BOS>\")[0],\n",
    "            \"<SEP>\": self.tokenizer.encode(\"<SEP>\")[0],\n",
    "            \"<EOS>\": self.tokenizer.encode(\"<EOS>\")[0],\n",
    "            \"<PAD>\": self.tokenizer.encode(\"<PAD>\")[0]\n",
    "        }\n",
    "\n",
    "        self.samples = []\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                q, a = item[\"prompt\"], item[\"completion\"]\n",
    "                tokens = self.encode_sample(q, a)\n",
    "                if len(tokens[\"input_ids\"]) <= self.max_length:\n",
    "                    self.samples.append(tokens)\n",
    "\n",
    "    def encode_sample(self, question, answer):\n",
    "        q_tokens = self.tokenizer.encode(question)\n",
    "        a_tokens = self.tokenizer.encode(answer)\n",
    "\n",
    "        input_ids = (\n",
    "            [self.special_tokens[\"<BOS>\"]] + \n",
    "            q_tokens + \n",
    "            [self.special_tokens[\"<SEP>\"]] +\n",
    "            a_tokens +\n",
    "            [self.special_tokens[\"<EOS>\"]]\n",
    "        )\n",
    "\n",
    "        label_ids = input_ids[1:] + [-100]\n",
    "        ignore_length = len(q_tokens) + 1\n",
    "        label_ids[:ignore_length] = [-100] * ignore_length\n",
    "\n",
    "        # label_ids = (\n",
    "        #     [-100] * (1 + len(q_tokens) + 1) + \n",
    "        #     a_tokens +\n",
    "        #     [self.special_tokens[\"<EOS>\"]]\n",
    "        #  )\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"label_ids\": label_ids}     \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            batch = random.sample(self.samples, batch_size)\n",
    "        else:\n",
    "            batch = self.samples[:batch_size]\n",
    "        \n",
    "        max_len = max(len(sample[\"input_ids\"]) for sample in batch)\n",
    "        input_ids_batch = []\n",
    "        label_ids_batch = []\n",
    "        attention_mask_batch = []\n",
    "\n",
    "        for sample in batch:\n",
    "            pad_len = max_len - len(sample[\"input_ids\"])\n",
    "            input_ids = sample[\"input_ids\"] + [self.special_tokens[\"<PAD>\"]] * pad_len\n",
    "            label_ids = sample[\"label_ids\"] + [-100] * pad_len\n",
    "            attention_mask = [1] * len(sample[\"input_ids\"]) + [0] * pad_len\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            label_ids_batch.append(label_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            \n",
    "        input_ids_batch = torch.tensor(input_ids_batch)\n",
    "        label_ids_batch = torch.tensor(label_ids_batch)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch)\n",
    "\n",
    "        return input_ids_batch, label_ids_batch, attention_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f4e76-67dd-4229-afa7-f920caeba360",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SCRATCH - DELETE LATER. run it single time and watch results\n",
    "\n",
    "# model.generate(max_length=20, num_return_sequences=1, query=\"Should I take the job at Google?\")\n",
    "\n",
    "model = GPT(GPTConfig)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Confirm gibberish\n",
    "model.generate(max_length=15)\n",
    "\n",
    "# Load GPT2 weights\n",
    "model = model.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Confirm coherency\n",
    "model.generate(max_length=15)\n",
    "\n",
    "print(\"Embedding for '!'\", model.transformer.wte.weight.data[50256])\n",
    "\n",
    "# Load dataset, update vocab with new special tokens\n",
    "dataloader = QADataLoader(\"qa_small_dataset.jsonl\", max_length=512)\n",
    "tokenizer = dataloader.get_tokenizer()\n",
    "new_length = len(tokenizer.get_vocab())\n",
    "print(\"Before resizing...\", model.transformer.wte.weight.shape)\n",
    "model.resize_token_embeddings_and_tie_weights(new_length)\n",
    "\n",
    "print(\"Embedding for '!'\", model.transformer.wte.weight.data[50256])\n",
    "print(\"Embedding for '!'\", model.transformer.wte.weight.data[50257:])\n",
    "\n",
    "model.generate(max_length=15, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e680bc12-cf33-4e97-83cf-722b09285896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25603 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model resized Embedding(50261, 1024) and Linear(in_features=1024, out_features=50261, bias=False) layers to 50261\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "\n",
    "model = model.from_pretrained('gpt2-medium')\n",
    "\n",
    "dataloader = QADataLoader(\"qa_dataset.jsonl\", max_length=512)\n",
    "tokenizer = dataloader.get_tokenizer()\n",
    "new_length = len(tokenizer.get_vocab())\n",
    "# print(\"Before resizing...\", model.transformer.wte.weight.shape)\n",
    "model.resize_token_embeddings_and_tie_weights(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8efbda-c439-44f4-a63f-80a58fb66221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE SPECIAL TOKENS TO SEE HOW OFF IT IS\n",
    "\n",
    "model.lm_head.weight.data[-4:] = model.lm_head.weight.data[tokenizer.encode('.')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75b5f1-52e3-4bb9-aa9e-5a47d848fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "weights0 = model.lm_head.weight.data[0]\n",
    "weights1 = model.lm_head.weight.data[-4]\n",
    "\n",
    "# weights0 = model.transformer.wte.weight.data[0]\n",
    "# weights1 = model.transformer.wte.weight.data[-4]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(len(weights0)), weights0)\n",
    "plt.plot(range(len(weights1)), weights1)\n",
    "plt.show()\n",
    "\n",
    "# model.lm_head.weight[-1].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74a05d-8c59-48a8-8218-fd6e5012717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "weights0 = model.lm_head.weight.data[0]\n",
    "weights1 = model.lm_head.weight.data[-4]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(len(weights0)), weights0)\n",
    "plt.plot(range(len(weights1)), weights1)\n",
    "plt.show()\n",
    "\n",
    "# model.lm_head.weight[-1].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a84097-99fb-4a41-8d95-468af2de2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode(\"Hello world\")\n",
    "x = torch.tensor(x)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "logits, _ = model(x)\n",
    "\n",
    "gooz = logits[:, -1, :]\n",
    "gooz = gooz.squeeze(0).data\n",
    "gooz\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(len(gooz)), gooz)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe7b50-9541-4112-b0dd-9a8b04249e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287fc37-8644-461c-acc9-ae180f8234e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(gooz, dim=-1)\n",
    "torch.topk(probs, 10, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139d1ad-9547-43b2-9b79-e2654ef7383f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1caab5f-6b58-41d8-8b2a-15bedc24280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best tacos? Yes\n",
      "Best tacos? Yes.\n",
      "Best tacos? Yes.\n",
      "\n",
      "Best tacos? Yes.\n",
      "\n",
      "\n",
      "Best tacos? Yes.\n",
      "\n",
      "T\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tort\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tortilla\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tortilla lovers\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tortilla lovers and\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tortilla lovers and food\n",
      "Best tacos? Yes.\n",
      "\n",
      "Tortilla lovers and foodies\n"
     ]
    }
   ],
   "source": [
    "model.generate(max_length=14, device='cpu', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69d944-1585-4336-a58d-dd70f7f4fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.lm_head.weight.data[-4:])\n",
    "average = model.lm_head.weight[:-4].mean(dim=0)\n",
    "# model.lm_head.weight[-1].norm()\n",
    "model.lm_head.weight.data[-4:] = average\n",
    "print(model.lm_head.weight.data[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546923c1-6563-4da9-84e7-90d58d1c0495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "\tdevice = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.manual_seed(1337)\n",
    "\n",
    "# model = GPT(GPTConfig)\n",
    "\n",
    "# model = model.from_pretrained('gpt2-medium')\n",
    "\n",
    "# dataloader = QADataLoader(\"qa_dataset.jsonl\", max_length=512)\n",
    "# tokenizer = dataloader.get_tokenizer()\n",
    "# new_length = len(tokenizer.get_vocab())\n",
    "# print(\"Before resizing...\", model.transformer.wte.weight.shape)\n",
    "# model.resize_token_embeddings_and_tie_weights(new_length)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in range(5):\n",
    "    x, y, att_mask = dataloader.get_batch(4)\n",
    "    x, y, att_mask = x.to(device), y.to(device), att_mask.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y, att_mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132d3c3-8633-4688-a801-5433ea45f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, att = dataloader.get_batch(4)\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "dataloader.tokenizer.decode(x[0])\n",
    "# model.transformer.wte(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ac367-b67b-464b-b1b4-dfd7b0e82e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.tensor([50258]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a4d07-32ba-4665-8e4c-34b292d81317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "x[0], y[0]\n",
    "\n",
    "logits, _ = model(x[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de740e4-1ffd-4e7f-bb4e-abd8f7498e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd74cd0-a1a8-453e-be18-0d7d198a828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape\n",
    "\n",
    "# inputs = logits.view(-1, logits.size(-1))\n",
    "# labels = y[0]\n",
    "# inputs.shape, labels.shape\n",
    "\n",
    "x = torch.tensor([[40, 1842, 11311, 13]])\n",
    "logits, _ = model(x)\n",
    "inputs = logits.view(-1, logits.size(-1))\n",
    "\n",
    "# labels = torch.tensor([40, 1842, 11311])\n",
    "labels = torch.tensor([1842, 11311, 13, -100])\n",
    "\n",
    "F.cross_entropy(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb01c11-1480-4fa9-8afd-4c3732436d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.tokenizer.encode(\"I love chocolate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd607bc4-fe7c-4024-b03c-3817c46f34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class TestQADataLoader:\n",
    "    def __init__(self, filepath, max_length=512, shuffle=True):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.tokenizer.add_special_tokens({\n",
    "            \"bos_token\": \"<BOS>\",\n",
    "            \"eos_token\": \"<EOS>\",\n",
    "            \"sep_token\": \"<SEP>\",\n",
    "            \"pad_token\": \"<PAD>\"\n",
    "        })\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<BOS>\": self.tokenizer.encode(\"<BOS>\")[0],\n",
    "            \"<SEP>\": self.tokenizer.encode(\"<SEP>\")[0],\n",
    "            \"<EOS>\": self.tokenizer.encode(\"<EOS>\")[0],\n",
    "            \"<PAD>\": self.tokenizer.encode(\"<PAD>\")[0]\n",
    "        }\n",
    "\n",
    "        self.samples = []\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                q, a = item[\"prompt\"], item[\"completion\"]\n",
    "                tokens = self.encode_sample(q, a)\n",
    "                if len(tokens[\"input_ids\"]) <= self.max_length:\n",
    "                    self.samples.append(tokens)\n",
    "\n",
    "    def encode_sample(self, question, answer):\n",
    "        q_tokens = self.tokenizer.encode(question)\n",
    "        a_tokens = self.tokenizer.encode(answer)\n",
    "\n",
    "        input_ids = (\n",
    "            [self.special_tokens[\"<BOS>\"]] + \n",
    "            q_tokens + \n",
    "            [self.special_tokens[\"<SEP>\"]] +\n",
    "            a_tokens +\n",
    "            [self.special_tokens[\"<EOS>\"]]\n",
    "        )\n",
    "\n",
    "        label_ids = input_ids[1:] + [-100]\n",
    "        ignore_length = len(q_tokens) + 1\n",
    "        label_ids[:ignore_length] = [-100] * ignore_length\n",
    "\n",
    "        \n",
    "        # label_ids[:len(q_tokens)+1] = [-100] * len(q_tokens)+1\n",
    "\n",
    "        # label_ids = (\n",
    "        #     [-100] * (1 + len(q_tokens) + 1) + \n",
    "        #     a_tokens +\n",
    "        #     [self.special_tokens[\"<EOS>\"]]\n",
    "        #  )\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"label_ids\": label_ids}     \n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            batch = random.sample(self.samples, batch_size)\n",
    "        else:\n",
    "            batch = self.samples[:batch_size]\n",
    "        \n",
    "        max_len = max(len(sample[\"input_ids\"]) for sample in batch)\n",
    "        input_ids_batch = []\n",
    "        label_ids_batch = []\n",
    "        attention_mask_batch = []\n",
    "\n",
    "        for sample in batch:\n",
    "            pad_len = max_len - len(sample[\"input_ids\"])\n",
    "            input_ids = sample[\"input_ids\"] + [self.special_tokens[\"<PAD>\"]] * pad_len\n",
    "            label_ids = sample[\"label_ids\"] + [-100] * pad_len\n",
    "            attention_mask = [1] * len(sample[\"input_ids\"]) + [0] * pad_len\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            label_ids_batch.append(label_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            \n",
    "        input_ids_batch = torch.tensor(input_ids_batch)\n",
    "        label_ids_batch = torch.tensor(label_ids_batch)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch)\n",
    "\n",
    "        return input_ids_batch, label_ids_batch, attention_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28505e-e528-4af4-ad02-39bca9ca8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = TestQADataLoader('qa_small_dataset.jsonl')\n",
    "gooz = testdata.get_batch(4)\n",
    "gooz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4ef4-abdb-484f-8420-786b8cc8be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "gooz[0]['label_ids']\n",
    "# testdata.tokenizer.decode(gooz[0]['label_ids'][:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
